{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain langchain_openai langchain_experimental langsmith pandas"
      ],
      "metadata": {
        "id": "ieEc9oxLJ-2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_if_undefined(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
        "\n",
        "\n",
        "_set_if_undefined(\"OPENAI_API_KEY\")\n",
        "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
        "_set_if_undefined(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Optional, add tracing in LangSmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\""
      ],
      "metadata": {
        "id": "FCjUcamGKMLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbdcdb0-9303-4196-da32-c9e58f9169af",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your OPENAI_API_KEY··········\n",
            "Please provide your LANGCHAIN_API_KEY··········\n",
            "Please provide your TAVILY_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "# This executes code locally, which can be unsafe\n",
        "python_repl_tool = PythonREPLTool()"
      ],
      "metadata": {
        "id": "DNnlLt32NvE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]}"
      ],
      "metadata": {
        "id": "fUS2NbA0N1av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel\n",
        "from typing import Literal\n",
        "\n",
        "# Define the members and system prompt\n",
        "members = [\"Researcher\", \"Coder\"]\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers: {members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\"\n",
        ")\n",
        "\n",
        "# Define the options for the next step\n",
        "options = [\"FINISH\"] + members\n",
        "\n",
        "# Correct usage of Literal without unpacking\n",
        "class RouteResponse(BaseModel):\n",
        "    next: Literal['FINISH', 'Researcher', 'Coder']  # Pass options directly\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Given the conversation above, who should act next?\"\n",
        "            \" Or should we FINISH? Select one of: {options}\",\n",
        "        ),\n",
        "    ]\n",
        ").partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Define the supervisor agent function\n",
        "def supervisor_agent(state):\n",
        "    supervisor_chain = (\n",
        "        prompt\n",
        "        | llm.with_structured_output(RouteResponse)\n",
        "    )\n",
        "    return supervisor_chain.invoke(state)\n",
        "\n",
        "# Example usage of supervisor_agent function\n",
        "# Example state can be defined here based on your use case\n",
        "# state = {\"messages\": ... }\n",
        "# response = supervisor_agent(state)\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "mqblHE27Oi45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import operator\n",
        "from typing import Sequence, TypedDict, Annotated\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Define the base class for the agent's state\n",
        "class AgentState(TypedDict):\n",
        "    # This annotation indicates that new messages will be added to the sequence\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    # This field represents where to route next\n",
        "    next: str\n",
        "\n",
        "# Create the research agent node\n",
        "research_agent = create_react_agent(llm, tools=[tavily_tool])\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "# Create the coder agent node\n",
        "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
        "code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
        "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
        "\n",
        "# Define supervisor_chain properly using prompt and llm as per your setup\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.with_structured_output(RouteResponse)  # Ensure RouteResponse is defined\n",
        ")\n",
        "\n",
        "# Create the workflow graph and add nodes\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"Coder\", code_node)\n",
        "workflow.add_node(\"supervisor\", supervisor_chain)  # Now supervisor_chain is defined\n"
      ],
      "metadata": {
        "id": "zSZwk0JGPD0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for member in members:\n",
        "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
        "    workflow.add_edge(member, \"supervisor\")\n",
        "# The supervisor populates the \"next\" field in the graph state\n",
        "# which routes to a node or finishes\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map[\"FINISH\"] = END\n",
        "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
        "# Finally, add entrypoint\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "P5V7eUToPFyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI  # Make sure to import ChatOpenAI correctly\n",
        "\n",
        "# Update your ChatOpenAI instance to use a valid model name\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")  # Use 'gpt-4' if you have access\n",
        "\n",
        "# Example usage of the graph\n",
        "for s in graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX6VfwNKP2sO",
        "outputId": "502f2d5a-df33-4182-a66a-13d8aecee5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'supervisor': {'next': 'Coder'}}\n",
            "----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Coder': {'messages': [HumanMessage(content='The code has been executed and printed \"Hello, World!\" to the terminal.', name='Coder')]}}\n",
            "----\n",
            "{'supervisor': {'next': 'FINISH'}}\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"Research on super sampling algorithms and give me code trapping rain water\")]},\n",
        "    {\"recursion_limit\": 100},\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdb81FmmXn58",
        "outputId": "a2dcb65b-c3c9-4da6-fabd-6d7e0e643e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'supervisor': {'next': 'Researcher'}}\n",
            "----\n",
            "{'Researcher': {'messages': [HumanMessage(content='### Research on Super Sampling Algorithms\\n\\n1. **[Intelligent Algorithm for Super-Sampling](https://link.springer.com/chapter/10.1007/978-3-030-05234-8_10)**:\\n   This paper presents a novel framework for super-sampling an image. It introduces intelligent interpolation in the intensity domain for learning-based super-resolution methods, contrasting with traditional antialiasing methods.\\n\\n2. **[NVIDIA DLSS (Deep Learning Super Sampling)](https://developer.nvidia.com/rtx/dlss)**:\\n   NVIDIA DLSS is a neural graphics technology that enhances performance using AI. It creates new frames, displays higher resolutions through image reconstruction, and improves image quality, particularly in ray-traced content.\\n\\n3. **[Deep Learning Super Sampling Research Paper](https://arxiv.org/pdf/2012.09810)**:\\n   This paper discusses a method for video super-resolution (SR) using low-resolution aliased frame images and their motion vectors. The approach aims to produce high-quality SR videos by compensating for motion between frames.\\n\\n4. **[Deep Learning Super Sampling on Wikipedia](https://en.wikipedia.org/wiki/Deep_learning_super_sampling)**:\\n   DLSS is a family of real-time deep learning image enhancement and upscaling technologies developed by Nvidia. It allows most of the graphics pipeline to run at a lower resolution for increased performance, then uses AI to upscale to higher resolutions.\\n\\n5. **[Neural Supersampling by Facebook](https://research.facebook.com/blog/2020/07/introducing-neural-supersampling-for-real-time-rendering/)**:\\n   This approach uses neural networks to restore sharp details while saving computational overhead. It achieves significant supersampling (16x) of rendered content with high spatial quality, ideal for real-time applications.\\n\\n### Code for Trapping Rain Water Problem\\n\\nHere is a Java implementation of the Trapping Rain Water problem:\\n\\n```java\\npublic class Solution {\\n    public int trap(int[] height) {\\n        if (height == null || height.length == 0) return 0;\\n        \\n        int n = height.length;\\n        int left = 0, right = n - 1;\\n        int leftMax = 0, rightMax = 0;\\n        int waterTrapped = 0;\\n        \\n        while (left < right) {\\n            if (height[left] < height[right]) {\\n                if (height[left] >= leftMax) {\\n                    leftMax = height[left];\\n                } else {\\n                    waterTrapped += leftMax - height[left];\\n                }\\n                left++;\\n            } else {\\n                if (height[right] >= rightMax) {\\n                    rightMax = height[right];\\n                } else {\\n                    waterTrapped += rightMax - height[right];\\n                }\\n                right--;\\n            }\\n        }\\n        \\n        return waterTrapped;\\n    }\\n\\n    public static void main(String[] args) {\\n        Solution sol = new Solution();\\n        int[] height = {0,1,0,2,1,0,1,3,2,1,2,1};\\n        System.out.println(\"Trapped water: \" + sol.trap(height)); // Output: 6\\n    }\\n}\\n```\\n\\nThis code uses the two-pointer technique to solve the problem efficiently. It maintains two pointers, one starting from the left and the other from the right, and moves them towards each other. It also keeps track of the maximum heights encountered from both directions and calculates the trapped water based on these heights.', name='Researcher')]}}\n",
            "----\n",
            "{'supervisor': {'next': 'FINISH'}}\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3E3akdiBXsnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}